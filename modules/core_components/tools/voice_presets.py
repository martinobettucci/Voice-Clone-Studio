"""
Voice Presets Tab

Use Qwen3-TTS pre-trained models or custom trained models with style control.
"""
# Setup path for standalone testing BEFORE imports
if __name__ == "__main__":
    import sys
    from pathlib import Path
    project_root = Path(__file__).parent.parent.parent.parent
    sys.path.insert(0, str(project_root))

import sys
from pathlib import Path
import gradio as gr
import soundfile as sf
from datetime import datetime
from textwrap import dedent
from pathlib import Path
from gradio_filelister import FileLister

from modules.core_components.tool_base import Tool, ToolConfig
from modules.core_components.ai_models.tts_manager import get_tts_manager
from modules.core_components.emotion_manager import process_save_emotion_result, process_delete_emotion_result
from modules.core_components.ui_components.prompt_assistant import (
    create_prompt_assistant,
    wire_prompt_assistant_events,
    wire_prompt_apply_listener,
)


class VoicePresetsTool(Tool):
    """Voice Presets tool implementation."""

    config = ToolConfig(
        name="Voice Presets",
        module_name="tool_voice_presets",
        description="Generate with trained models or Qwen3's Style-Controlled Premium Speakers",
        enabled=True,
        category="generation"
    )

    @classmethod
    def create_tool(cls, shared_state):
        """Create Voice Presets tool UI."""
        components = {}

        # Get helper functions and config
        get_trained_models = shared_state['get_trained_models']
        create_qwen_advanced_params = shared_state['create_qwen_advanced_params']
        _user_config = shared_state['_user_config']
        _active_emotions = shared_state['_active_emotions']
        CUSTOM_VOICE_SPEAKERS = shared_state['CUSTOM_VOICE_SPEAKERS']
        MODEL_SIZES_CUSTOM = shared_state['MODEL_SIZES_CUSTOM']
        LANGUAGES = shared_state['LANGUAGES']
        show_input_modal_js = shared_state['show_input_modal_js']
        show_confirmation_modal_js = shared_state['show_confirmation_modal_js']
        save_emotion_handler = shared_state['save_emotion_handler']
        delete_emotion_handler = shared_state['delete_emotion_handler']
        save_preference = shared_state['save_preference']
        format_help_html = shared_state['format_help_html']
        get_sample_choices = shared_state['get_sample_choices']
        get_dataset_folders = shared_state['get_dataset_folders']
        get_dataset_files = shared_state['get_dataset_files']
        DATASETS_DIR = shared_state['DATASETS_DIR']
        confirm_trigger = shared_state['confirm_trigger']
        input_trigger = shared_state['input_trigger']

        with gr.TabItem("Voice Presets", id="tab_voice_presets") as voice_presets_tab:
            components['voice_presets_tab'] = voice_presets_tab
            gr.Markdown("Use your Qwen3-TTS trained models or Qwen3's Speakers with style control")

            initial_voice_type = _user_config.get("voice_type", "Trained Models")
            is_premium = (initial_voice_type.strip() == "Qwen Speakers")

            components['voice_type_radio'] = gr.Radio(
                choices=["Trained Models", "Qwen Speakers"],
                value=initial_voice_type,
                show_label=False,
                container=False,
                label="Voice Source"
            )

            with gr.Row():
                # Left - Speaker selection
                with gr.Column(scale=1):
                    gr.Markdown("### Select Voice Type")
                    # Qwen Speakers dropdown
                    components['speaker_section'] = gr.Column(visible=is_premium)
                    with components['speaker_section']:
                        speaker_choices = CUSTOM_VOICE_SPEAKERS
                        components['custom_speaker_dropdown'] = gr.Dropdown(
                            choices=speaker_choices,
                            label="Speaker",
                            info="Choose a Qwen speaker voice"
                        )

                        components['custom_model_size'] = gr.Dropdown(
                            choices=MODEL_SIZES_CUSTOM,
                            value=_user_config.get("custom_voice_size", "Large"),
                            label="Model",
                            info="Small = faster, Large = better quality",
                            scale=1
                        )

                        premium_speaker_guide = dedent("""\
                            **Qwen Speakers:**

                            | Speaker | Voice | Language |
                            |---------|-------|----------|
                            | Vivian | Bright young female    | ðŸ‡¨ðŸ‡³ Chinese |
                            | Serena | Warm gentle female    | ðŸ‡¨ðŸ‡³ Chinese |
                            | Uncle_Fu | Seasoned mellow male    | ðŸ‡¨ðŸ‡³ Chinese |
                            | Dylan | Youthful Beijing male    | ðŸ‡¨ðŸ‡³ Chinese |
                            | Eric | Lively Chengdu male    | ðŸ‡¨ðŸ‡³ Chinese |
                            | Ryan | Dynamic male | ðŸ‡ºðŸ‡¸ English    |
                            | Aiden | Sunny American male    | ðŸ‡ºðŸ‡¸ English |
                            | Ono_Anna | Playful female    | ðŸ‡¯ðŸ‡µ Japanese |
                            | Sohee | Warm female    | ðŸ‡°ðŸ‡· Korean |

                            *Each speaker works best in native language.*
                            """)

                        gr.HTML(
                            value=format_help_html(premium_speaker_guide, height="auto"),
                            container=True,
                            padding=True
                        )
                    # Trained models dropdown
                    components['trained_section'] = gr.Column(visible=not is_premium)
                    with components['trained_section']:
                        def get_initial_model_list():
                            """Get initial list of trained models for dropdown initialization."""
                            models = get_trained_models()
                            if not models:
                                return ["(No trained models found)"]
                            return ["(Select Model)"] + [m['display_name'] for m in models]

                        def refresh_trained_models():
                            """Refresh model list."""
                            models = get_trained_models()
                            if not models:
                                return gr.update(choices=["(No trained models found)"], value="(No trained models found)")
                            choices = ["(Select Model)"] + [m['display_name'] for m in models]
                            return gr.update(choices=choices, value="(Select Model)")

                        initial_choices = get_initial_model_list()
                        initial_value = initial_choices[0]

                        components['trained_model_dropdown'] = gr.Dropdown(
                            choices=initial_choices,
                            value=initial_value,
                            label="Trained Model",
                            info="Select your custom trained voice"
                        )

                        components['refresh_trained_btn'] = gr.Button("Refresh", size="sm", visible=False)

                        # ICL (In-Context Learning) mode for enhanced voice cloning
                        components['icl_enabled'] = gr.Checkbox(
                            label="Enable ICL (Experimental)",
                            value=False,
                            info="Use a voice sample for more expressive results"
                        )

                        components['icl_sample_section'] = gr.Column(visible=False)
                        with components['icl_sample_section']:
                            components['icl_dataset_dropdown'] = gr.Dropdown(
                                choices=["(Select Dataset)"] + get_dataset_folders(),
                                value="(Select Dataset)",
                                label="Dataset",
                                info="Select the dataset used for training",
                                interactive=True
                            )
                            components['icl_refresh_datasets'] = gr.Button("Refresh Datasets", size="sm")
                            components['icl_voice_lister'] = FileLister(
                                value=[],
                                height=150,
                                show_footer=False,
                                interactive=True,
                            )

                            components['icl_audio_preview'] = gr.Audio(
                                label="Preview",
                                type="filepath",
                                interactive=False,
                                elem_id="icl-audio-preview"
                            )

                        trained_models_tip = dedent("""\
                        **Trained Models:**

                        Custom voices you've trained in the Train Model tab.

                        **ICL Mode (Enhanced Voice Clone):**
                        Select a sample from your training dataset for more
                        expressive results. The model uses both its training
                        and the reference audio for better voice similarity.

                        *Tip: Later epochs are usually better trained*
                        """)
                        gr.HTML(
                            value=format_help_html(trained_models_tip, height="auto"),
                            container=True,
                            padding=True,
                        )

                # Right - Generation
                with gr.Column(scale=3):
                    gr.Markdown("### Generate Speech")

                    components['custom_text_input'] = gr.Textbox(
                        label="Text to Generate",
                        placeholder="Enter the text you want spoken...",
                        lines=6
                    )

                    components['custom_instruct_input'] = gr.Textbox(
                        label="Style Instructions (Optional)",
                        placeholder="e.g., 'Speak with excitement' or 'Very sad and slow' or 'ç”¨æ„¤æ€’çš„è¯­æ°”è¯´'",
                        lines=2,
                        info="Control emotion, tone, speed, etc.",
                        visible=is_premium
                    )
                    components['prompt_assistant'] = create_prompt_assistant(
                        shared_state=shared_state,
                        target_ids=["voice_presets.text", "voice_presets.style"],
                        default_target_id="voice_presets.text",
                    )

                    with gr.Row():
                        components['custom_language'] = gr.Dropdown(
                            choices=LANGUAGES,
                            value=_user_config.get("language", "Auto"),
                            label="Language",
                            scale=2
                        )
                        components['custom_seed'] = gr.Number(
                            label="Seed (-1 for random)",
                            value=-1,
                            precision=0,
                            scale=1
                        )

                    # Qwen Advanced Parameters (always visible, emotion controls match voice type)
                    emotion_visible = not is_premium

                    custom_params = create_qwen_advanced_params(
                        emotions_dict=_active_emotions,
                        include_emotion=True,
                        initial_emotion="(None)",
                        initial_intensity=1.0,
                        visible=True,
                        emotion_visible=emotion_visible,
                        shared_state=shared_state
                    )

                    # Store emotion row references for visibility toggling
                    components['custom_emotion_row'] = custom_params.get('emotion_row')
                    components['custom_emotion_buttons_row'] = custom_params.get('emotion_buttons_row')

                    # Create alias references for backward compatibility
                    components['custom_emotion_preset'] = custom_params['emotion_preset']
                    components['custom_emotion_intensity'] = custom_params['emotion_intensity']
                    components['custom_save_emotion_btn'] = custom_params.get('save_emotion_btn')
                    components['custom_delete_emotion_btn'] = custom_params.get('delete_emotion_btn')
                    components['custom_emotion_save_name'] = custom_params.get('emotion_save_name')
                    components['custom_do_sample'] = custom_params['do_sample']
                    components['custom_temperature'] = custom_params['temperature']
                    components['custom_top_k'] = custom_params['top_k']
                    components['custom_top_p'] = custom_params['top_p']
                    components['custom_repetition_penalty'] = custom_params['repetition_penalty']
                    components['custom_max_new_tokens'] = custom_params['max_new_tokens']
                    components['custom_params'] = custom_params

                    components['custom_generate_btn'] = gr.Button("Generate Audio", variant="primary", size="lg")

                    components['custom_output_audio'] = gr.Audio(
                        label="Generated Audio",
                        type="filepath"
                    )
                    components['preset_status'] = gr.Textbox(label="Status", max_lines=5, interactive=False)

        return components

    @classmethod
    def setup_events(cls, components, shared_state):
        """Wire up Voice Presets tab events."""

        # Get helper functions and directories
        get_trained_models = shared_state['get_trained_models']
        get_sample_choices = shared_state['get_sample_choices']
        get_dataset_folders = shared_state['get_dataset_folders']
        get_dataset_files = shared_state['get_dataset_files']
        DATASETS_DIR = shared_state['DATASETS_DIR']
        show_input_modal_js = shared_state['show_input_modal_js']
        show_confirmation_modal_js = shared_state['show_confirmation_modal_js']
        save_emotion_handler = shared_state['save_emotion_handler']
        delete_emotion_handler = shared_state['delete_emotion_handler']
        save_preference = shared_state['save_preference']
        confirm_trigger = shared_state['confirm_trigger']
        input_trigger = shared_state['input_trigger']
        prompt_apply_trigger = shared_state.get('prompt_apply_trigger')
        OUTPUT_DIR = shared_state['OUTPUT_DIR']
        play_completion_beep = shared_state.get('play_completion_beep')
        user_config = shared_state.get('_user_config', {})

        # Get TTS manager (singleton)
        tts_manager = get_tts_manager()

        custom_params = components['custom_params']

        def generate_custom_voice_handler(text_to_generate, language, speaker, instruct, seed, model_size="1.7B",
                                          do_sample=True, temperature=0.9, top_k=50, top_p=1.0,
                                          repetition_penalty=1.05, max_new_tokens=2048, progress=gr.Progress()):
            """Generate audio using the CustomVoice model with premium speakers."""
            if not text_to_generate or not text_to_generate.strip():
                return None, "âŒ Please enter text to generate."

            if not speaker:
                return None, "âŒ Please select a speaker."

            try:
                progress(0.1, desc=f"Loading CustomVoice model ({model_size})...")

                audio_data, sr = tts_manager.generate_custom_voice(
                    text=text_to_generate,
                    language=language,
                    speaker=speaker,
                    instruct=instruct,
                    model_size=model_size,
                    seed=int(seed) if seed is not None else -1,
                    do_sample=do_sample,
                    temperature=temperature,
                    top_k=top_k,
                    top_p=top_p,
                    repetition_penalty=repetition_penalty,
                    max_new_tokens=max_new_tokens
                )

                progress(0.8, desc="Saving audio...")
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_file = OUTPUT_DIR / f"custom_{speaker}_{timestamp}.wav"

                sf.write(str(output_file), audio_data, sr)

                # Save metadata file
                metadata_file = output_file.with_suffix(".txt")
                metadata = dedent(f"""\
                    Generated: {timestamp}
                    Type: Custom Voice
                    Model: CustomVoice {model_size}
                    Speaker: {speaker}
                    Language: {language}
                    Seed: {seed}
                    Instruct: {instruct.strip() if instruct else ''}
                    Text: {' '.join(text_to_generate.split())}
                    """)
                metadata_out = '\n'.join(line.lstrip() for line in metadata.lstrip().splitlines())
                metadata_file.write_text(metadata_out, encoding="utf-8")

                progress(1.0, desc="Done!")
                instruct_msg = f" with style: {instruct.strip()[:30]}..." if instruct and instruct.strip() else ""
                if play_completion_beep:
                    play_completion_beep()
                return str(output_file), f"Audio saved to: {output_file.name}\nSpeaker: {speaker}{instruct_msg}\nSeed: {seed} | {model_size}"

            except Exception as e:
                import traceback
                traceback.print_exc()
                return None, f"âŒ Error generating audio: {str(e)}"

        def generate_with_trained_model_handler(text_to_generate, language, speaker_name, checkpoint_path, instruct, seed,
                                                do_sample=True, temperature=0.9, top_k=50, top_p=1.0,
                                                repetition_penalty=1.05, max_new_tokens=2048,
                                                icl_enabled=False, icl_dataset=None, icl_sample_name=None,
                                                progress=gr.Progress()):
            """Generate audio using a trained custom voice model checkpoint."""
            if not text_to_generate or not text_to_generate.strip():
                return None, "âŒ Please enter text to generate."

            # Resolve ICL voice sample from dataset if enabled
            voice_sample_path = None
            ref_text = None
            if icl_enabled:
                if not icl_dataset or icl_dataset in ["(Select Dataset)", ""]:
                    return None, "âŒ Please select a dataset for ICL mode."
                if not icl_sample_name or icl_sample_name.strip() == "":
                    return None, "âŒ Please select a voice sample for ICL mode."

                audio_path = DATASETS_DIR / icl_dataset / icl_sample_name
                if not audio_path.exists():
                    return None, f"âŒ Audio file not found: {icl_sample_name}"

                voice_sample_path = str(audio_path)

                # Look for matching .txt transcript file
                txt_path = audio_path.with_suffix(".txt")
                if txt_path.exists():
                    ref_text = txt_path.read_text(encoding="utf-8").strip()

                if not ref_text or not ref_text.strip():
                    return None, (
                        f"âŒ No transcript found for '{icl_sample_name}' in dataset '{icl_dataset}'.\n\n"
                        "Make sure the sample has a matching .txt file with the transcript."
                    )

            try:
                mode_desc = "ICL mode" if icl_enabled and voice_sample_path else "speaker embedding"
                progress(0.1, desc=f"Loading trained model ({mode_desc})...")

                # Call tts_manager method
                audio_data, sr = tts_manager.generate_with_trained_model(
                    text=text_to_generate,
                    language=language,
                    speaker_name=speaker_name,
                    checkpoint_path=checkpoint_path,
                    instruct=instruct if not icl_enabled else None,
                    seed=int(seed) if seed is not None else -1,
                    do_sample=do_sample,
                    temperature=temperature,
                    top_k=top_k,
                    top_p=top_p,
                    repetition_penalty=repetition_penalty,
                    max_new_tokens=max_new_tokens,
                    user_config=user_config,
                    icl_mode=icl_enabled and voice_sample_path is not None,
                    voice_sample_path=voice_sample_path,
                    ref_text=ref_text,
                )

                progress(0.8, desc="Saving audio...")
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_file = OUTPUT_DIR / f"trained_{speaker_name}_{timestamp}.wav"

                sf.write(str(output_file), audio_data, sr)

                # Save metadata file
                metadata_file = output_file.with_suffix(".txt")
                icl_active = icl_enabled and voice_sample_path is not None
                metadata = dedent(f"""\
                    Generated: {timestamp}
                    Type: Trained Model{' (ICL)' if icl_active else ''}
                    Model: {checkpoint_path}
                    Speaker: {speaker_name}
                    Language: {language}
                    Seed: {seed}
                    ICL Mode: {icl_active}
                    ICL Dataset: {icl_dataset if icl_active else 'N/A'}
                    ICL Sample: {icl_sample_name if icl_active else 'N/A'}
                    Instruct: {instruct.strip() if instruct and not icl_enabled else ''}
                    Text: {' '.join(text_to_generate.split())}
                    """)
                metadata_out = '\n'.join(line.lstrip() for line in metadata.lstrip().splitlines())
                metadata_file.write_text(metadata_out, encoding="utf-8")

                progress(1.0, desc="Done!")
                instruct_msg = f" with style: {instruct.strip()[:30]}..." if instruct and instruct.strip() and not icl_enabled else ""
                icl_msg = f" | ICL: {icl_dataset}/{icl_sample_name}" if icl_enabled and voice_sample_path else ""
                if play_completion_beep:
                    play_completion_beep()
                return str(output_file), f"Audio saved: {output_file.name}\nSpeaker: {speaker_name}{instruct_msg}{icl_msg}\nSeed: {seed} | Trained Model"

            except Exception as e:
                import traceback
                traceback.print_exc()
                return None, f"âŒ Error generating audio: {str(e)}"

        def extract_speaker_name(selection):
            """Extract speaker name from dropdown selection."""
            if not selection:
                return None
            return selection.split(" - ")[0].split(" (")[0]

        def toggle_voice_type(voice_type):
            """Toggle between premium and trained model sections."""
            is_premium = voice_type == "Qwen Speakers"

            if is_premium:
                # Return in order: speaker_section, trained_section, instruct_input, emotion_row, emotion_buttons_row,
                # emotion_preset, emotion_intensity, temperature, top_p, repetition_penalty
                return (
                    gr.update(visible=True),   # speaker_section
                    gr.update(visible=False),  # trained_section
                    gr.update(visible=True),   # instruct_input
                    gr.update(visible=False),  # emotion_row
                    gr.update(visible=False),  # emotion_buttons_row
                    gr.update(value=None),     # emotion_preset
                    gr.update(value=1.0),      # emotion_intensity
                    gr.update(value=0.9),      # temperature
                    gr.update(value=1.0),      # top_p
                    gr.update(value=1.05)      # repetition_penalty
                )
            else:
                return (
                    gr.update(visible=False),  # speaker_section
                    gr.update(visible=True),   # trained_section
                    gr.update(visible=False),  # instruct_input
                    gr.update(visible=True),   # emotion_row
                    gr.update(visible=True),   # emotion_buttons_row
                    gr.update(),               # emotion_preset
                    gr.update(),               # emotion_intensity
                    gr.update(),               # temperature
                    gr.update(),               # top_p
                    gr.update()                # repetition_penalty
                )

        def generate_with_voice_type(text, lang, speaker_sel, instruct, seed, model_size, voice_type, premium_speaker, trained_model,
                                     do_sample, temperature, top_k, top_p, repetition_penalty, max_new_tokens,
                                     icl_enabled=False, icl_dataset=None, icl_lister_value=None, progress=gr.Progress()):
            """Generate audio with either premium or trained voice."""
            icl_sample_name = get_selected_icl_filename(icl_lister_value) if icl_lister_value else None

            if voice_type == "Qwen Speakers":
                speaker = extract_speaker_name(premium_speaker)
                if not speaker:
                    return None, "âŒ Please select a premium speaker"

                return generate_custom_voice_handler(
                    text, lang, speaker, instruct, seed,
                    "1.7B" if model_size == "Large" else "0.6B",
                    do_sample, temperature, top_k, top_p, repetition_penalty, max_new_tokens,
                    progress
                )
            else:
                if not trained_model or trained_model in ["(No trained models found)", "(Select Model)"]:
                    return None, "âŒ Please select a trained model or train one first"

                models = get_trained_models()
                model_path = None
                speaker_name = None
                for model in models:
                    if model['display_name'] == trained_model:
                        model_path = model['path']
                        speaker_name = model['speaker_name']
                        break

                if not model_path:
                    return None, f"âŒ Model not found: {trained_model}"

                return generate_with_trained_model_handler(
                    text, lang, speaker_name, model_path, instruct, seed,
                    do_sample, temperature, top_k, top_p, repetition_penalty, max_new_tokens,
                    icl_enabled, icl_dataset, icl_sample_name,
                    progress
                )

        # Only wire events for components that exist (not None)
        if components.get('voice_type_radio') is not None:
            outputs = [
                components['speaker_section'], components['trained_section'],
                components['custom_instruct_input'],
                components.get('custom_emotion_row'),
                components.get('custom_emotion_buttons_row'),
                components['custom_emotion_preset'], components['custom_emotion_intensity'],
                components['custom_temperature'], components['custom_top_p'], components['custom_repetition_penalty']
            ]
            # Filter out None values
            outputs = [o for o in outputs if o is not None]

            components['voice_type_radio'].change(
                toggle_voice_type,
                inputs=[components['voice_type_radio']],
                outputs=outputs
            )

        if components.get('prompt_assistant'):
            wire_prompt_assistant_events(
                assistant=components['prompt_assistant'],
                target_components={
                    "voice_presets.text": components['custom_text_input'],
                    "voice_presets.style": components['custom_instruct_input'],
                },
                status_component=components['preset_status'],
                shared_state=shared_state,
            )

        if prompt_apply_trigger is not None:
            wire_prompt_apply_listener(
                prompt_apply_trigger=prompt_apply_trigger,
                target_components={
                    "voice_presets.text": components['custom_text_input'],
                    "voice_presets.style": components['custom_instruct_input'],
                },
                status_component=components['preset_status'],
            )

        # Auto-refresh trained models when tab is selected
        components['voice_presets_tab'].select(
            lambda: (
                gr.update(choices=["(Select Model)"] + [m['display_name'] for m in get_trained_models()] if get_trained_models() else ["(No trained models found)"])
            ),
            outputs=[components['trained_model_dropdown']]
        )

        # ICL toggle: show/hide voice sample section
        components['icl_enabled'].change(
            lambda enabled: gr.update(visible=enabled),
            inputs=[components['icl_enabled']],
            outputs=[components['icl_sample_section']]
        )

        def get_selected_icl_filename(lister_value):
            """Extract selected filename from FileLister value."""
            if not lister_value:
                return None
            selected = lister_value.get("selected", [])
            if len(selected) == 1:
                return selected[0]
            return None

        # ICL dataset change: update sample lister
        def update_icl_samples(folder):
            """Update ICL sample lister when dataset changes."""
            files = get_dataset_files(folder)
            return files, None

        components['icl_dataset_dropdown'].change(
            update_icl_samples,
            inputs=[components['icl_dataset_dropdown']],
            outputs=[components['icl_voice_lister'], components['icl_audio_preview']]
        )

        # ICL refresh datasets
        components['icl_refresh_datasets'].click(
            lambda: gr.update(choices=["(Select Dataset)"] + get_dataset_folders(), value="(Select Dataset)"),
            outputs=[components['icl_dataset_dropdown']]
        )

        # ICL sample preview on selection
        def load_icl_audio_preview(lister_value, folder):
            """Load ICL audio preview from FileLister selection."""
            filename = get_selected_icl_filename(lister_value)
            if not folder or not filename or folder in ("(No folders)", "(Select Dataset)"):
                return None
            audio_path = DATASETS_DIR / folder / filename
            if audio_path.exists():
                return str(audio_path)
            return None

        components['icl_voice_lister'].change(
            load_icl_audio_preview,
            inputs=[components['icl_voice_lister'], components['icl_dataset_dropdown']],
            outputs=[components['icl_audio_preview']]
        )

        # Double-click = play preview
        components['icl_voice_lister'].double_click(
            fn=None,
            js="() => { setTimeout(() => { const btn = document.querySelector('#icl-audio-preview .play-pause-button'); if (btn) btn.click(); }, 150); }"
        )

        # Apply emotion preset to Custom Voice parameters
        if 'update_from_emotion' in components.get('custom_params', {}):
            components['custom_emotion_preset'].change(
                components['custom_params']['update_from_emotion'],
                inputs=[components['custom_emotion_preset'], components['custom_emotion_intensity']],
                outputs=[components['custom_temperature'], components['custom_top_p'], components['custom_repetition_penalty']]
            )

            components['custom_emotion_intensity'].change(
                components['custom_params']['update_from_emotion'],
                inputs=[components['custom_emotion_preset'], components['custom_emotion_intensity']],
                outputs=[components['custom_temperature'], components['custom_top_p'], components['custom_repetition_penalty']]
            )

        # Emotion management buttons
        components['custom_save_emotion_btn'].click(
            fn=None,
            inputs=[components['custom_emotion_preset']],
            outputs=None,
            js=show_input_modal_js(
                title="Save Emotion Preset",
                message="Enter a name for this emotion preset:",
                placeholder="e.g., Happy, Sad, Excited",
                context="custom_emotion_"
            )
        )

        def handle_custom_emotion_input(input_value, intensity, temp, rep_pen, top_p):
            """Process input modal submission for Voice Presets emotion save."""
            if not input_value or not input_value.startswith("custom_emotion_"):
                return gr.update(), gr.update()

            parts = input_value.split("_")
            if len(parts) >= 3:
                if parts[2] == "cancel":
                    return gr.update(), ""
                emotion_name = "_".join(parts[2:-1])

                # Use shared helper to process save result
                save_result = save_emotion_handler(emotion_name, intensity, temp, rep_pen, top_p)
                return process_save_emotion_result(save_result, shared_state)

            return gr.update(), gr.update()

        components['custom_delete_emotion_btn'].click(
            fn=None,
            inputs=None,
            outputs=None,
            js=show_confirmation_modal_js(
                title="Delete Emotion Preset?",
                message="This will permanently delete this emotion preset from your configuration.",
                confirm_button_text="Delete",
                context="custom_emotion_"
            )
        )

        components['custom_generate_btn'].click(
            generate_with_voice_type,
            inputs=[
                components['custom_text_input'], components['custom_language'], components['custom_speaker_dropdown'],
                components['custom_instruct_input'], components['custom_seed'], components['custom_model_size'],
                components['voice_type_radio'], components['custom_speaker_dropdown'], components['trained_model_dropdown'],
                components['custom_do_sample'], components['custom_temperature'], components['custom_top_k'], components['custom_top_p'],
                components['custom_repetition_penalty'], components['custom_max_new_tokens'],
                components['icl_enabled'], components['icl_dataset_dropdown'], components['icl_voice_lister']
            ],
            outputs=[components['custom_output_audio'], components['preset_status']]
        )

        def delete_custom_emotion_wrapper(confirm_value, emotion_name):
            """Only process if context matches custom_emotion_."""
            if not confirm_value or not confirm_value.startswith("custom_emotion_"):
                return gr.update(), gr.update()

            # Call the delete handler and discard the clear_trigger (3rd value)
            delete_result = delete_emotion_handler(confirm_value, emotion_name)
            dropdown_update, status_msg, _clear = process_delete_emotion_result(delete_result, shared_state)
            return dropdown_update, status_msg

        confirm_trigger.change(
            delete_custom_emotion_wrapper,
            inputs=[confirm_trigger, components['custom_emotion_preset']],
            outputs=[components['custom_emotion_preset'], components['preset_status']]
        )

        input_trigger.change(
            handle_custom_emotion_input,
            inputs=[input_trigger, components['custom_emotion_intensity'], components['custom_temperature'], components['custom_repetition_penalty'], components['custom_top_p']],
            outputs=[components['custom_emotion_preset'], components['preset_status']]
        )

        # Refresh emotion dropdowns when tab is selected
        components['voice_presets_tab'].select(
            lambda: gr.update(choices=shared_state['get_emotion_choices'](shared_state['_active_emotions'])),
            outputs=[components['custom_emotion_preset']]
        )

        # Save preferences when settings change
        components['custom_model_size'].change(
            lambda x: save_preference("custom_voice_size", x),
            inputs=[components['custom_model_size']],
            outputs=[]
        )

        components['custom_language'].change(
            lambda x: save_preference("language", x),
            inputs=[components['custom_language']],
            outputs=[]
        )

        components['voice_type_radio'].change(
            lambda x: save_preference("voice_type", x),
            inputs=[components['voice_type_radio']],
            outputs=[]
        )

# Export for tab registry
get_tool_class = lambda: VoicePresetsTool

if __name__ == "__main__":
    """Standalone testing of Voice Presets tool."""
    from modules.core_components.tools import run_tool_standalone
    run_tool_standalone(VoicePresetsTool, port=7863, title="Voice Presets - Standalone")
